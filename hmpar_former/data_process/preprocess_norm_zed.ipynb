{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6ba0af-dd87-478e-89c3-e2522e9b3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cac3ff2-b0f0-4395-afcf-fba8d008a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f59882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_06.pkl\n",
      "Data shape: (47, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_04.pkl\n",
      "Data shape: (39, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_01.pkl\n",
      "Data shape: (49, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_10.pkl\n",
      "Data shape: (44, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_11.pkl\n",
      "Data shape: (41, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_09.pkl\n",
      "Data shape: (37, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_05.pkl\n",
      "Data shape: (45, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_07.pkl\n",
      "Data shape: (54, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_08.pkl\n",
      "Data shape: (45, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_03.pkl\n",
      "Data shape: (40, 1)\n",
      "Loaded file:  /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/24_09_14/Khoa_02.pkl\n",
      "Data shape: (55, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "raw_data_path  = '/home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/raw_data/'\n",
    "fileroot='24_09_09/'\n",
    "\n",
    "# Create an empty numpy array to store the dataset\n",
    "datasets = []\n",
    "for file in os.listdir(raw_data_path+fileroot):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        data_pkl = load_dataset(raw_data_path + fileroot + file)\n",
    "        print(\"Loaded file: \", raw_data_path + fileroot + file)\n",
    "        # trimp the data\n",
    "        n = 0 # number of frame from the start\n",
    "        m = 10 # number of frame from the end\n",
    "        data_pkl = {key: value[n:-m] if m > 0 else value[n:] for key, value in data_pkl.items()}\n",
    "        print(\"Data shape:\", data_pkl['timestamp'].shape)\n",
    "        datasets.append(data_pkl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e29f574-fbf6-4e95-a5e9-99feffa196e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined statistics:\n",
      "  Position medians shape: (6, 3)\n",
      "  Position IQRs shape: (6, 3)\n",
      "  Velocity medians shape: (6, 3)\n",
      "  Velocity IQRs shape: (6, 3)\n",
      "1\n",
      "(47, 6, 3)\n",
      "Calculated and normalized for dataset1:\n",
      "  Position shape: (47, 6, 3)\n",
      "  Velocity shape: (47, 6, 3)\n",
      "  Acceleration shape: (47, 6, 3)\n",
      "\n",
      "2\n",
      "(39, 6, 3)\n",
      "Calculated and normalized for dataset2:\n",
      "  Position shape: (39, 6, 3)\n",
      "  Velocity shape: (39, 6, 3)\n",
      "  Acceleration shape: (39, 6, 3)\n",
      "\n",
      "3\n",
      "(49, 6, 3)\n",
      "Calculated and normalized for dataset3:\n",
      "  Position shape: (49, 6, 3)\n",
      "  Velocity shape: (49, 6, 3)\n",
      "  Acceleration shape: (49, 6, 3)\n",
      "\n",
      "4\n",
      "(44, 6, 3)\n",
      "Calculated and normalized for dataset4:\n",
      "  Position shape: (44, 6, 3)\n",
      "  Velocity shape: (44, 6, 3)\n",
      "  Acceleration shape: (44, 6, 3)\n",
      "\n",
      "5\n",
      "(41, 6, 3)\n",
      "Calculated and normalized for dataset5:\n",
      "  Position shape: (41, 6, 3)\n",
      "  Velocity shape: (41, 6, 3)\n",
      "  Acceleration shape: (41, 6, 3)\n",
      "\n",
      "6\n",
      "(37, 6, 3)\n",
      "Calculated and normalized for dataset6:\n",
      "  Position shape: (37, 6, 3)\n",
      "  Velocity shape: (37, 6, 3)\n",
      "  Acceleration shape: (37, 6, 3)\n",
      "\n",
      "7\n",
      "(45, 6, 3)\n",
      "Calculated and normalized for dataset7:\n",
      "  Position shape: (45, 6, 3)\n",
      "  Velocity shape: (45, 6, 3)\n",
      "  Acceleration shape: (45, 6, 3)\n",
      "\n",
      "8\n",
      "(54, 6, 3)\n",
      "Calculated and normalized for dataset8:\n",
      "  Position shape: (54, 6, 3)\n",
      "  Velocity shape: (54, 6, 3)\n",
      "  Acceleration shape: (54, 6, 3)\n",
      "\n",
      "9\n",
      "(45, 6, 3)\n",
      "Calculated and normalized for dataset9:\n",
      "  Position shape: (45, 6, 3)\n",
      "  Velocity shape: (45, 6, 3)\n",
      "  Acceleration shape: (45, 6, 3)\n",
      "\n",
      "10\n",
      "(40, 6, 3)\n",
      "Calculated and normalized for dataset10:\n",
      "  Position shape: (40, 6, 3)\n",
      "  Velocity shape: (40, 6, 3)\n",
      "  Acceleration shape: (40, 6, 3)\n",
      "\n",
      "11\n",
      "(55, 6, 3)\n",
      "Calculated and normalized for dataset11:\n",
      "  Position shape: (55, 6, 3)\n",
      "  Velocity shape: (55, 6, 3)\n",
      "  Acceleration shape: (55, 6, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def robust_normalize_data_with_clipping(data, medians_per_joint_axis, iqrs_per_joint_axis, normalized_data, clipping_percentiles=(1, 99)):\n",
    "    for joint in range(data.shape[1]):  # For each joint\n",
    "        for axis in range(data.shape[2]):  # For each axis (x, y, z)\n",
    "            joint_axis_data = data[:, joint, axis]\n",
    "            # Determine clipping thresholds based on percentiles\n",
    "            lower_threshold, upper_threshold = np.percentile(joint_axis_data, clipping_percentiles)\n",
    "            # Clip the data based on thresholds\n",
    "            clipped_values = np.clip(joint_axis_data, lower_threshold, upper_threshold)\n",
    "            # Normalize the clipped data, avoiding division by zero\n",
    "            if iqrs_per_joint_axis[joint, axis] > 0:\n",
    "                normalized_values = (clipped_values - medians_per_joint_axis[joint, axis]) / iqrs_per_joint_axis[joint, axis]\n",
    "            else:\n",
    "                normalized_values = clipped_values  # Keep original values if IQR is 0\n",
    "            # Store the normalized values\n",
    "            normalized_data[:, joint, axis] = normalized_values\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def calculate_combined_statistics(data_list):\n",
    "    combined_data = np.concatenate(data_list, axis=0)\n",
    "    medians = np.median(combined_data, axis=0)\n",
    "    q75, q25 = np.percentile(combined_data, [75, 25], axis=0)\n",
    "    iqrs = q75 - q25\n",
    "    return medians, iqrs\n",
    "\n",
    "def process_datasets_with_combined_normalization(datasets):\n",
    "    results = {}\n",
    "    pos_list, vel_list, acc_list = [], [], []\n",
    "    for dataset in datasets:\n",
    "        pos_list.append(dataset['points'])\n",
    "        vel_list.append(dataset['velocity'])\n",
    "        acc_list.append(dataset['acceleration'])\n",
    "\n",
    "    # Calculate combined statistics\n",
    "    medians_pos, iqrs_pos = calculate_combined_statistics(pos_list)\n",
    "    medians_vel, iqrs_vel = calculate_combined_statistics(vel_list)\n",
    "    medians_acc, iqrs_acc = calculate_combined_statistics(acc_list)\n",
    "\n",
    "    print(\"Combined statistics:\")\n",
    "    print(f\"  Position medians shape: {medians_pos.shape}\")\n",
    "    print(f\"  Position IQRs shape: {iqrs_pos.shape}\")\n",
    "    print(f\"  Velocity medians shape: {medians_vel.shape}\")\n",
    "    print(f\"  Velocity IQRs shape: {iqrs_vel.shape}\")\n",
    "\n",
    "        \n",
    "    for i, (pos, vel, acc) in enumerate(zip(pos_list, vel_list, acc_list), 1):\n",
    "        print(i)\n",
    "        print(pos.shape)\n",
    "        norm_pos = np.empty_like(pos)\n",
    "        norm_vel = np.empty_like(vel)\n",
    "        norm_acc = np.empty_like(acc)\n",
    "        # print(pos.shape[0])\n",
    "        \n",
    "        norm_pos = robust_normalize_data_with_clipping(pos, medians_pos, iqrs_pos, norm_pos)\n",
    "        norm_vel = robust_normalize_data_with_clipping(vel, medians_vel, iqrs_vel, norm_vel)\n",
    "        norm_acc = robust_normalize_data_with_clipping(acc, medians_acc, iqrs_acc, norm_acc)\n",
    "\n",
    "        results[f\"dataset{i}_normpos\"] = norm_pos\n",
    "        results[f\"dataset{i}_normvel\"] = norm_vel\n",
    "        results[f\"dataset{i}_normacc\"] = norm_acc\n",
    "\n",
    "        print(f\"Calculated and normalized for dataset{i}:\")\n",
    "        print(f\"  Position shape: {norm_pos.shape}\")\n",
    "        print(f\"  Velocity shape: {norm_vel.shape}\")\n",
    "        print(f\"  Acceleration shape: {norm_acc.shape}\")\n",
    "        print()\n",
    "\n",
    "    # Store the combined statistics\n",
    "    results[\"combined_medians_pos\"] = medians_pos\n",
    "    results[\"combined_iqrs_pos\"] = iqrs_pos\n",
    "    results[\"combined_medians_vel\"] = medians_vel\n",
    "    results[\"combined_iqrs_vel\"] = iqrs_vel\n",
    "    results[\"combined_medians_acc\"] = medians_acc\n",
    "    results[\"combined_iqrs_acc\"] = iqrs_acc\n",
    "\n",
    "    return results\n",
    "\n",
    "# Usage example\n",
    "\n",
    "results = process_datasets_with_combined_normalization(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65d3c292-575a-4c0a-a65a-fcfdea75ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=dataset2_pos\n",
    "# norm_data=results[\"dataset2_normpos\"]\n",
    "# # Plot the original and normalized data for a specific joint and axis\n",
    "# joint, axis = 0, 0  # Change as needed\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(data[:, joint, axis], bins=20, alpha=0.7, label='Original')\n",
    "# plt.title(\"Original Data Distribution\")\n",
    "# plt.xlabel(\"Value\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(norm_data[:, joint, axis], bins=20, alpha=0.7, label='Normalized')\n",
    "# plt.title(\"Normalized Data Distribution\")\n",
    "# plt.xlabel(\"Value\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Check the median and range of the normalized data\n",
    "# normalized_median = np.nanmedian(norm_data[:, joint, axis])\n",
    "# print(\"Median of normalized data:\", normalized_median)\n",
    "\n",
    "# within_iqr = ((norm_data[:, joint, axis] > -2) & (norm_data[:, joint, axis] < 2)).sum()\n",
    "# print(f\"Data points within [-1, 1] (IQR): {within_iqr} out of {norm_data.shape[0]}\")\n",
    "\n",
    "# within_iqr2 = ((data[:, joint, axis] > -1) & (data[:, joint, axis] < 1)).sum()\n",
    "# print(f\"Original Data points within [-1, 1] (IQR): {within_iqr2} out of {norm_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21df9440-e671-453a-b55a-4e2c771dabb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/process_data/24_09_14/training_raw.pkl\n",
      "Results saved to /home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/process_data/24_09_14/training_raw_normalize.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def save_results_to_pickle(results, filename):\n",
    "#     with open(filename, 'wb') as f:\n",
    "#         pickle.dump(results, f)\n",
    "\n",
    "#Add to the training DIC instead of new dic?\n",
    "unnormalised = {}\n",
    "for i in range(1, len(datasets)+1):\n",
    "    unnormalised[f\"dataset{i}_pos\"] = datasets[i-1]['points']\n",
    "    unnormalised[f\"dataset{i}_vel\"] = datasets[i-1]['velocity']\n",
    "    unnormalised[f\"dataset{i}_acc\"] = datasets[i-1]['acceleration'] \n",
    "\n",
    "process_data_path = '/home/augustine/lfd_ws/src/skill_transfer/hmpar_former/data_process/process_data/'\n",
    "file_root = '24_09_09/'\n",
    "filename1 = 'training_raw.pkl'\n",
    "filename2 = 'training_raw_normalize.pkl'\n",
    "# Save the results\n",
    "with open(process_data_path + file_root + filename1, 'wb') as f:\n",
    "    pickle.dump(unnormalised, f)\n",
    "print(f\"Results saved to {process_data_path + file_root + filename1}\")\n",
    "\n",
    "# Save the results\n",
    "with open(process_data_path + file_root + filename2, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"Results saved to {process_data_path + file_root + filename2}\")\n",
    "\n",
    "\n",
    "# Print the data\n",
    "# for key, value in results.items():\n",
    "#     print(f\"Key: {key}\")\n",
    "#     print(f\"Shape: {value.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31cfc5-a304-4fa7-97d5-4c31cebd5eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d24ff-21a6-4002-9274-5db741b84be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
